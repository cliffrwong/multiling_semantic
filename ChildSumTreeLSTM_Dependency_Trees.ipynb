{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Cross-lingual \"Paraphrase\" Classifer using Child-Sum TreeLSTMs\n",
    "\n",
    "Currently only for German-English and Spanish-English language pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# boilerplate\n",
    "import codecs\n",
    "import functools\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "import tempfile\n",
    "import zipfile\n",
    "import scipy.stats\n",
    "import pandas as pd\n",
    "from nltk.tree import Tree\n",
    "from nltk.parse import DependencyGraph\n",
    "\n",
    "from nltk.tokenize import sexpr\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "import tensorflow as tf\n",
    "import tensorflow_fold as td\n",
    "# sess = tf.Session(config=tf.ConfigProto(\n",
    "#     allow_soft_placement=True, log_device_placement=True))\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File path for the [bilingual word embeddings](http://www.aclweb.org/anthology/W15-1521)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_dir = '/home/cliffrwong/Documents/code/\n",
    "bipara_dir = os.path.join(data_dir, 'bipara/')\n",
    "treelstm_dir = os.path.join(data_dir, 'treelstm/')\n",
    "\n",
    "# Choose between German (de) or Spanish (es) language pairs\n",
    "lang = \"de\"\n",
    "\n",
    "if lang == \"de\":\n",
    "    sem_dir = os.path.join(treelstm_dir, \"data/de_dep/\")\n",
    "    bipara_dir += \"de/\"\n",
    "    full_bipara_en_path = os.path.join(bipara_dir, 'unsup.512.en')\n",
    "    full_bipara_de_path = os.path.join(bipara_dir, 'unsup.512.de')\n",
    "elif lang == \"es\":\n",
    "    sem_dir = treelstm_dir + \"data/es/\"\n",
    "    bipara_dir += \"es/\"\n",
    "    full_bipara_en_path = os.path.join(bipara_dir, 'es/out.en')\n",
    "    full_bipara_de_path = os.path.join(bipara_dir, 'es/out.es')\n",
    "else:\n",
    "    raise (\"language not available\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out words in the bilingual word2vec matrix that are not present in the training, dev, or test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if lang == \"de\":\n",
    "    parseLangDir = os.path.join(treelstm_dir,\"data/de/\")\n",
    "elif lang == \"es\":\n",
    "    parseLangDir = os.path.join(treelstm_dir,\"data/es\")\n",
    "else:\n",
    "    print('language not available')\n",
    "        \n",
    "def filter_bipara(curLang):\n",
    "    vocab = set()\n",
    "    for set1 in ['train', 'dev', 'test']:\n",
    "        parseDir = os.path.join(*[parseLangDir, set1, set1])\n",
    "        if curLang == \"en\":\n",
    "            set2 = \".src\"\n",
    "        else:\n",
    "            set2 = \".mt\"\n",
    "        parseFile = parseDir + set2\n",
    "        with open(parseFile, 'r') as fin:\n",
    "            for line in fin:\n",
    "                vocab.update([x.lower() for x in line.strip().replace('\\\\', '').split()])\n",
    "    nread = 0\n",
    "    nwrote = 0\n",
    "    if lang == \"de\":\n",
    "        full_bipara_path = os.path.join(bipara_dir, 'unsup.512.'+ curLang) \n",
    "    elif lang == \"es\":\n",
    "        full_bipara_path = os.path.join(bipara_dir, 'out.'+ curLang) \n",
    "\n",
    "    filtered_bipara_path = os.path.join(bipara_dir, 'filtered_bipara_{0}.txt'.format(curLang))\n",
    "    with codecs.open(full_bipara_path, encoding='utf-8') as f:\n",
    "        with codecs.open(filtered_bipara_path, 'w', encoding='utf-8') as out:\n",
    "            for line in f:\n",
    "                nread += 1\n",
    "                line = line.strip()\n",
    "                if not line: continue\n",
    "                if line.split(u' ', 1)[0] in vocab:\n",
    "                    out.write(line + '\\n')\n",
    "                    nwrote += 1\n",
    "    print('read %s lines, wrote %s' % (nread, nwrote))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Filter out words that don't appear in the dataset, since the full dataset is a bit large (5GB). This is purely a performance optimization and has no effect on the final results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 40055 lines, wrote 4727\n",
      "read 95196 lines, wrote 5152\n"
     ]
    }
   ],
   "source": [
    "filter_bipara('en')\n",
    "if lang == \"de\":\n",
    "    filter_bipara('de')\n",
    "elif lang == \"es\":\n",
    "    filter_bipara('es')\n",
    "else:\n",
    "    raise(\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dependParse = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Load the filtered word embeddings into a matrix and build an dict from words to indices into the matrix. Add a random embedding vector for out-of-vocabulary words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def load_embeddings(lang):\n",
    "  \"\"\"Loads embedings, returns weight matrix and dict from words to indices.\"\"\"\n",
    "  print('loading word embeddings from lang %s' % lang)\n",
    "  weight_vectors = []\n",
    "  word_idx = {}\n",
    "  embedding_path = os.path.join(bipara_dir, 'filtered_bipara_{0}.txt'.format(lang))\n",
    "  with codecs.open(embedding_path, encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "      word, vec = line.split(u' ', 1)\n",
    "      word_idx[word.lower()] = len(weight_vectors)\n",
    "      weight_vectors.append(np.array(vec.split(), dtype=np.float32))\n",
    "  # Random embedding vector for unknown words.\n",
    "  weight_vectors.append(np.random.uniform(\n",
    "      -0.05, 0.05, weight_vectors[0].shape).astype(np.float32))\n",
    "  return np.stack(weight_vectors), word_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word embeddings from lang en\n",
      "loading word embeddings from lang de\n"
     ]
    }
   ],
   "source": [
    "src_weight_matrix, src_word_idx = load_embeddings('en')\n",
    "if lang == \"de\":\n",
    "    target_weight_matrix, target_word_idx = load_embeddings('de')\n",
    "elif lang == \"es\":\n",
    "    target_weight_matrix, target_word_idx = load_embeddings('es')\n",
    "else:\n",
    "    raise('error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Load the Dependency Trees (from Google's SyntaxNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def readDepTree(set1):\n",
    "    data_dir = os.path.join(sem_dir, set1) + \"/\"\n",
    "    results = []\n",
    "    for fext in ['src', 'mt']:\n",
    "        setResult = []\n",
    "        fileIn = data_dir + set1 + \"_dep.\" + fext\n",
    "        with open(fileIn, 'r') as fin:\n",
    "            curSent = \"\"\n",
    "            for line in fin:\n",
    "                if not line.isspace():\n",
    "                    curSent += line\n",
    "                else:\n",
    "                    dg = DependencyGraph(curSent)\n",
    "                    tree = dg.tree()\n",
    "                    setResult.append(tree)\n",
    "                    curSent = \"\"\n",
    "        results.append(setResult)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_tree = readDepTree(\"train\")\n",
    "dev_tree = readDepTree(\"dev\")\n",
    "test_tree = readDepTree(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Generate the training examples from the dependency trees and convert the label HTER values into an array representation. See TreeLSTM paper for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 5\n",
    "def convDist(y):\n",
    "    y = float(y)/25\n",
    "    if y >= 4:\n",
    "        return [0,0,0,0,1]\n",
    "    result = [0]*NUM_CLASSES\n",
    "    y_bar = math.floor(y)\n",
    "    result[int(y_bar)+1] = y - y_bar\n",
    "    result[int(y_bar)] = y_bar - y + 1\n",
    "    return result\n",
    "\n",
    "def load_examples(tree, dataSet):\n",
    "    examples = []\n",
    "    filename = sem_dir + dataSet + \"/\" + dataSet + \".hter\"\n",
    "    with codecs.open(filename, encoding='utf-8') as f:\n",
    "        for aTree, bTree, line in zip(tree[0], tree[1], f):\n",
    "            line = float(line)\n",
    "            if lang == \"de\":\n",
    "                examples.append((min(line, 100), convDist(line), \n",
    "                            aTree, bTree))\n",
    "            elif lang == \"es\":\n",
    "                examples.append((min(line*100, 100), convDist(100*line), \n",
    "                            aTree, bTree))\n",
    "            \n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_trees = load_examples(train_tree, \"train\")\n",
    "dev_trees = load_examples(dev_tree, \"dev\")\n",
    "test_trees = load_examples(test_tree, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(press\n",
      "  (reverse To (direction the (scrolling of)))\n",
      "  ,\n",
      "  (key the minus sign ( - ))\n",
      "  .)\n"
     ]
    }
   ],
   "source": [
    "print(train_trees[0][2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2. Build the Model (Tensorflow Fold)\n",
    "\n",
    "The part of the model deals with building the TreeLSTM to represent the input text as a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ChildSumTreeLSTMCell(tf.contrib.rnn.BasicLSTMCell):\n",
    "\n",
    "  def __init__(self, num_units, keep_prob=1.0):\n",
    "    super(ChildSumTreeLSTMCell, self).__init__(num_units)\n",
    "    self._keep_prob = keep_prob\n",
    "\n",
    "  def __call__(self, inputs, state, scope=None):\n",
    "    with tf.variable_scope(scope or type(self).__name__):\n",
    "        child_h_sum, fc = state  \n",
    "        concat = tf.contrib.layers.linear(\n",
    "            tf.concat([inputs, child_h_sum], 1), 3 * self._num_units)\n",
    "    \n",
    "        i, u, o = tf.split(value=concat, num_or_size_splits=3, axis=1)\n",
    "      \n",
    "        u = self._activation(u)\n",
    "        if not isinstance(self._keep_prob, float) or self._keep_prob < 1:\n",
    "            u = tf.nn.dropout(u, self._keep_prob)\n",
    "\n",
    "        new_c = fc + (tf.sigmoid(i) * u)\n",
    "        new_h = self._activation(new_c) * tf.sigmoid(o)\n",
    "\n",
    "        new_state = tf.contrib.rnn.LSTMStateTuple(new_c, new_h)\n",
    "      \n",
    "        return new_h, new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "keep_prob_ph = tf.placeholder_with_default(1.0, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The LSTM Layer with the ChildSumLSTMCell. Change the \"name_or_scope\" to be the same or different for the src and tgt trees for shared or separate parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lstm_num_units = 150\n",
    "src_tree_lstm = td.ScopedLayer(\n",
    "      tf.contrib.rnn.DropoutWrapper(\n",
    "          ChildSumTreeLSTMCell(lstm_num_units, keep_prob=keep_prob_ph),\n",
    "          input_keep_prob=keep_prob_ph, output_keep_prob=keep_prob_ph),\n",
    "      name_or_scope='src_tree_lstm')\n",
    "\n",
    "tgt_tree_lstm = td.ScopedLayer(\n",
    "      tf.contrib.rnn.DropoutWrapper(\n",
    "          ChildSumTreeLSTMCell(lstm_num_units, keep_prob=keep_prob_ph),\n",
    "          input_keep_prob=keep_prob_ph, output_keep_prob=keep_prob_ph),\n",
    "      name_or_scope='src_tree_lstm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The word embedding for the src and target language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "src_word_embedding = td.Embedding(\n",
    "    *src_weight_matrix.shape, initializer=src_weight_matrix, name='src_word_embedding', trainable=False)\n",
    "\n",
    "target_word_embedding = td.Embedding(\n",
    "    *target_weight_matrix.shape, initializer=target_weight_matrix, name='tgt_word_embedding', trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward Declaration for recursively traversing the TreeLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "src_embed_subtree = td.ForwardDeclaration(name='src_embed_subtree')\n",
    "target_embed_subtree = td.ForwardDeclaration(name='target_embed_subtree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def combLinear(x, y, scope=None):\n",
    "    with tf.variable_scope(scope):\n",
    "        concat = tf.sigmoid(tf.contrib.layers.linear(\n",
    "              tf.concat([x, y], 1), lstm_num_units))\n",
    "    return concat\n",
    "    \n",
    "ffc = td.ScopedLayer(combLinear, name_or_scope='ffc')\n",
    "\n",
    "def tree2vec2():\n",
    "     return td.AllOf(td.GetItem(0), \n",
    "             td.AllOf(td.GetItem(1) >> td.GetItem(1) >> td.Sum(),\n",
    "                     td.AllOf(td.AllOf(td.GetItem(0) >> td.Broadcast(), \n",
    "                                       td.GetItem(1) >> td.GetItem(1)) >>\n",
    "                              td.Zip() >> td.Map(ffc),\n",
    "                              td.GetItem(1) >> td.GetItem(0)) >> \n",
    "                     td.Zip() >> td.Map(td.Function(tf.multiply)) >> td.Sum()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def src_word2vec():\n",
    "    unknown_idx = len(src_word_idx)\n",
    "    lookup_src_word = lambda word: src_word_idx.get(word.lower(), unknown_idx)  \n",
    "    return td.InputTransform(lookup_src_word) >> td.Scalar('int32') >> src_word_embedding\n",
    "\n",
    "def target_word2vec():\n",
    "    unknown_idx = len(target_word_idx)\n",
    "    lookup_target_word = lambda word: target_word_idx.get(word.lower(), unknown_idx)  \n",
    "    return td.InputTransform(lookup_target_word) >> td.Scalar('int32') >> target_word_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def zero_state():\n",
    "    return ((td.Zeros(src_tree_lstm.state_size,),) >>\n",
    "            td.AllOf(td.Map(td.GetItem(0)), td.Map(td.GetItem(1))))\n",
    "\n",
    "def src_logits_and_state():\n",
    "\n",
    "  leaf_case = td.AllOf(src_word2vec(), zero_state())\n",
    "  node_case = (td.AllOf(td.InputTransform(lambda x:x.label()) >> src_word2vec(), \n",
    "                        td.InputTransform(list) >> td.Map(src_embed_subtree()) >> \n",
    "                        td.AllOf(td.Map(td.GetItem(0)), td.Map(td.GetItem(1)))))\n",
    "                       \n",
    "  tree2vec = (td.OneOf(lambda x: str(type(x)), \n",
    "                       {\"<class 'nltk.tree.Tree'>\": node_case, \n",
    "                                \"<class 'str'>\":leaf_case} ))\n",
    "  return tree2vec >> tree2vec2() >> src_tree_lstm >> td.GetItem(1)\n",
    "\n",
    "def target_logits_and_state():\n",
    "\n",
    "  leaf_case = td.AllOf(target_word2vec(), zero_state())\n",
    "  node_case = (td.AllOf(td.InputTransform(lambda x:x.label()) >> target_word2vec(), \n",
    "                        td.InputTransform(list) >> td.Map(target_embed_subtree()) >> \n",
    "                        td.AllOf(td.Map(td.GetItem(0)), td.Map(td.GetItem(1)))))\n",
    "                       \n",
    "  tree2vec = (td.OneOf(lambda x: str(type(x)), \n",
    "                       {\"<class 'nltk.tree.Tree'>\": node_case, \n",
    "                                \"<class 'str'>\":leaf_case} ))\n",
    "  return tree2vec >> tree2vec2() >> tgt_tree_lstm >> td.GetItem(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model =  td.Record((td.Scalar('float32'), \n",
    "                    td.Vector(NUM_CLASSES, dtype='float32'), \n",
    "                    src_logits_and_state(), \n",
    "                    target_logits_and_state()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Resolve the forward declarations by calling this function when the forward declaraction var is encountered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "src_embed_subtree.resolve_to(src_logits_and_state())\n",
    "target_embed_subtree.resolve_to(target_logits_and_state())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Compile the tensorflow fold model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input type: PyObjectType()\n",
      "output type: TupleType(TensorType((), 'float32'), TensorType((5,), 'float32'), TupleType(TensorType((150,), 'float32'), TensorType((150,), 'float32')), TupleType(TensorType((150,), 'float32'), TensorType((150,), 'float32')))\n"
     ]
    }
   ],
   "source": [
    "compiler = td.Compiler.create(model)\n",
    "print('input type: %s' % model.input_type)\n",
    "print('output type: %s' % model.output_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3. Build the Model (Tensorflow)\n",
    "\n",
    "This part of the model deals with the classifier layer and calculating the losses and other metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.03\n",
    "\n",
    "KEEP_PROB = 1.0\n",
    "BATCH_SIZE = 25\n",
    "EPOCHS = 20\n",
    "REGLAMBDA = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Use the Adagrad Optimizer from the TreeLSTM paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_feed_dict = {keep_prob_ph: KEEP_PROB}\n",
    "opt = tf.train.AdagradOptimizer(LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Return the outputs to Tensorflow to build the rest of the model for the classifier layer and calculate the losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"ffc/fully_connected/weights/read:0\", shape=(662, 150), dtype=float32)\n",
      "Tensor(\"ffc/fully_connected/biases/read:0\", shape=(150,), dtype=float32)\n",
      "Tensor(\"src_tree_lstm/fully_connected/weights/read:0\", shape=(662, 450), dtype=float32)\n",
      "Tensor(\"src_tree_lstm/fully_connected/biases/read:0\", shape=(450,), dtype=float32)\n",
      "Tensor(\"src_tree_lstm_2/fully_connected/weights/read:0\", shape=(662, 450), dtype=float32)\n",
      "Tensor(\"src_tree_lstm_2/fully_connected/biases/read:0\", shape=(450,), dtype=float32)\n",
      "Tensor(\"hs_FC/weights/read:0\", shape=(300, 50), dtype=float32)\n",
      "Tensor(\"hs_FC/biases/read:0\", shape=(50,), dtype=float32)\n",
      "Tensor(\"pth_FC/weights/read:0\", shape=(50, 5), dtype=float32)\n",
      "Tensor(\"pth_FC/biases/read:0\", shape=(5,), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cliffrwong/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "label, pk, lVecC, lVecH, rVecC, rVecH = compiler.output_tensors\n",
    "hx = lVecH * rVecH\n",
    "hplus = tf.abs(lVecH - rVecH)\n",
    "temp = tf.concat([hx,hplus], 1)\n",
    "\n",
    "hs = tf.contrib.layers.fully_connected(temp, 50, \n",
    "                                       activation_fn=tf.nn.sigmoid,\n",
    "                                        biases_initializer=tf.constant_initializer(0.01),\n",
    "                                        scope=\"hs_FC\")\n",
    "pth = tf.contrib.layers.fully_connected(hs, NUM_CLASSES, \n",
    "                                       activation_fn=tf.nn.softmax,\n",
    "                                        biases_initializer=tf.constant_initializer(0.01),\n",
    "                                        scope=\"pth_FC\")\n",
    "                    \n",
    "\n",
    "\n",
    "l2 = tf.constant(REGLAMBDA/2) * sum(\n",
    "    tf.nn.l2_loss(tf_var)\n",
    "        for tf_var in tf.trainable_variables() \n",
    "        if not (\"bias\" in tf_var.name)\n",
    "        )\n",
    "for tf_var in tf.trainable_variables():\n",
    "    print(tf_var)\n",
    "\n",
    "diff = tf.log(pk+tf.constant(1e-5))-tf.log(pth+tf.constant(1e-5))\n",
    "loss = tf.reduce_sum(pk*diff)+l2\n",
    "\n",
    "grads_and_vars = opt.compute_gradients(loss)\n",
    "train = opt.apply_gradients(grads_and_vars)\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate predicted value, the mean average error, and the room mean square error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "r = tf.constant(list(range(0,101,25)), dtype='float32')\n",
    "pred = tf.reduce_sum(tf.multiply(r, pth), axis=1)\n",
    "mae = tf.reduce_mean(tf.abs(label-pred))\n",
    "rmse = tf.sqrt(tf.reduce_mean((label-pred)*(label-pred)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The TF graph is now complete; initialize the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Start by defining a function that does a single step of training on a batch and returns the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train_step(batch):\n",
    "  train_feed_dict[compiler.loom_input_tensor] = batch\n",
    "  _, batch_loss = sess.run([train, loss], train_feed_dict)\n",
    "  return batch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now similarly for an entire epoch of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train_epoch(train_set_local):\n",
    "  return sum(train_step(batch) for batch in td.group_by_batches(train_set_local, BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the training and development set data into loom inputs for tensorflow fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_set = compiler.build_loom_inputs(train_trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dev_feed_dict = compiler.build_feed_dict(dev_trees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Evaluate the dev set with Pearson r, Spearman rho, mae, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def dev_eval(epoch, train_loss):\n",
    "  dev_mae, dev_rmse, dev_label, dev_pred, dev_loss = sess.run([mae, rmse, label, pred, loss], dev_feed_dict)\n",
    "\n",
    "  pearson = scipy.stats.pearsonr(dev_pred, dev_label)\n",
    "  spearman = scipy.stats.spearmanr(dev_pred, dev_label)\n",
    "\n",
    "  print('dev results: pearson: %.3e, spearman: %.3e, mae: %.3e, rmse: %.3e, loss: %.3e, train_loss: %.3e'\n",
    "        % (pearson[0], spearman[0], dev_mae, dev_rmse, dev_loss, train_loss))\n",
    "  return pearson[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This loop trains the model and saves the best seen model so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev results: pearson: 2.944e-01, spearman: 3.194e-01, mae: 1.457e+01, rmse: 1.983e+01, loss: 7.449e+02, train_loss: 9.602e+03\n",
      "model saved in file: /home/cliffrwong/Documents/code/testTF_Fold/models/qualest_similarity_model-1\n",
      "dev results: pearson: 3.175e-01, spearman: 3.546e-01, mae: 1.460e+01, rmse: 1.934e+01, loss: 7.118e+02, train_loss: 9.383e+03\n",
      "model saved in file: /home/cliffrwong/Documents/code/testTF_Fold/models/qualest_similarity_model-2\n",
      "dev results: pearson: 3.242e-01, spearman: 3.556e-01, mae: 1.436e+01, rmse: 2.000e+01, loss: 7.492e+02, train_loss: 9.092e+03\n",
      "model saved in file: /home/cliffrwong/Documents/code/testTF_Fold/models/qualest_similarity_model-3\n",
      "dev results: pearson: 3.669e-01, spearman: 4.006e-01, mae: 1.453e+01, rmse: 1.896e+01, loss: 7.019e+02, train_loss: 8.798e+03\n",
      "model saved in file: /home/cliffrwong/Documents/code/testTF_Fold/models/qualest_similarity_model-4\n",
      "dev results: pearson: 3.680e-01, spearman: 3.984e-01, mae: 1.416e+01, rmse: 1.915e+01, loss: 7.046e+02, train_loss: 8.465e+03\n",
      "model saved in file: /home/cliffrwong/Documents/code/testTF_Fold/models/qualest_similarity_model-5\n",
      "dev results: pearson: 3.918e-01, spearman: 4.115e-01, mae: 1.444e+01, rmse: 1.970e+01, loss: 7.759e+02, train_loss: 8.038e+03\n",
      "model saved in file: /home/cliffrwong/Documents/code/testTF_Fold/models/qualest_similarity_model-6\n",
      "dev results: pearson: 3.715e-01, spearman: 3.891e-01, mae: 1.496e+01, rmse: 1.926e+01, loss: 7.435e+02, train_loss: 7.499e+03\n",
      "dev results: pearson: 3.806e-01, spearman: 4.034e-01, mae: 1.555e+01, rmse: 2.001e+01, loss: 7.763e+02, train_loss: 6.797e+03\n",
      "dev results: pearson: 3.486e-01, spearman: 3.559e-01, mae: 1.484e+01, rmse: 1.966e+01, loss: 8.026e+02, train_loss: 5.937e+03\n",
      "dev results: pearson: 3.551e-01, spearman: 3.712e-01, mae: 1.518e+01, rmse: 2.003e+01, loss: 8.512e+02, train_loss: 4.942e+03\n",
      "dev results: pearson: 3.408e-01, spearman: 3.586e-01, mae: 1.529e+01, rmse: 2.079e+01, loss: 9.679e+02, train_loss: 3.973e+03\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-23fde6ec1b6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffled\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshuffled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0mdev_pearson\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdev_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mdev_pearson\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_dev_pearson\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-55037103bdb3>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(train_set_local)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set_local\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup_by_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set_local\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-32-55037103bdb3>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set_local\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup_by_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set_local\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-31-f9dac2895704>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mtrain_feed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcompiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloom_input_tensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_feed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cliffrwong/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cliffrwong/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cliffrwong/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/cliffrwong/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cliffrwong/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_dev_pearson = 0.0\n",
    "data_dir = '/home/cliffrwong/Documents/code/testTF_Fold/models/'\n",
    "save_path = os.path.join(data_dir, 'qualest_similarity_model')\n",
    "\n",
    "for epoch, shuffled in enumerate(td.epochs(train_set, EPOCHS, shuffle=True), 1):\n",
    "  train_loss = train_epoch(shuffled)\n",
    "  dev_pearson = dev_eval(epoch, train_loss)\n",
    "  if dev_pearson > best_dev_pearson:\n",
    "    best_dev_pearson = dev_pearson\n",
    "    checkpoint_path = saver.save(sess, save_path, global_step=epoch)\n",
    "    print('model saved in file: %s' % checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Evaulate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "saver.restore(sess, checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "See how we did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test results: pearson: 3.559e-01, spearman: 3.767e-01, mae: 1.426e+01, rmse: 1.918e+01, loss: 1.606e+03\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats\n",
    "\n",
    "test_feed_dict = compiler.build_feed_dict(test_trees)\n",
    "test_mae, test_rmse, test_label, test_pred, test_loss = sess.run([mae, rmse, label, pred, loss], test_feed_dict)\n",
    "\n",
    "pearson = scipy.stats.pearsonr(test_pred, test_label)\n",
    "spearman = scipy.stats.spearmanr(test_pred, test_label)\n",
    "\n",
    "print('test results: pearson: %.3e, spearman: %.3e, mae: %.3e, rmse: %.3e, loss: %.3e'\n",
    "        % (pearson[0], spearman[0], test_mae, test_rmse, test_loss))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
